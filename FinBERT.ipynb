{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tqdm\n",
    "import yfinance as yf\n",
    "\n",
    "import data_read\n",
    "import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Fine-tune bert on Autolabeled dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.02ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")\\n\\nfrom sklearn import metrics\\nimport numpy as np \\n\\ndef compute_metrics(eval_pred):\\n    logits, labels = eval_pred\\n    predictions = np.argmax(logits, axis=-1)\\n    return metrics.accuracy_score(predictions, labels)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=fin_dataset_train,\\n    compute_metrics=compute_metrics,\\n)\\n\\ntrainer.train()'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases, labels = data_read.load_fin_pharsebank()\n",
    "(fin_phrases_train, fin_phrases_test, fin_y_train, fin_y_test) =\\\n",
    "     train_test_split(phrases, labels, test_size = 0.2, random_state = 3)\n",
    "\n",
    "fin_dataset_train = {}\n",
    "fin_dataset_train['labels'] = fin_y_train\n",
    "fin_dataset_train['phrase'] = fin_phrases_train\n",
    "fin_dataset_train = Dataset.from_dict(fin_dataset_train)\n",
    "fin_dataset_train = fin_dataset_train.map(\n",
    "    lambda examples:  tokenizer(examples[\"phrase\"], padding=\"max_length\", max_length=100, truncation = True, add_special_tokens = True),\n",
    "    batched = True\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metrics.accuracy_score(predictions, labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=fin_dataset_train,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = pipeline(\"sentiment-analysis\", model = model, tokenizer=tokenizer)\n",
    "results = finbert(fin_phrases_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_y_pred = [1 if result['label'] == 'positive' else 0 for result in results]\n",
    "print(\"classification report:\\n\", metrics.classification_report(fin_y_test,fin_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' -- Generate our automatically-labelled Reuters dataset -- '''\n",
    "headers = data_read.parse_news_data()\n",
    "\n",
    "# Get S&P500 tickers and price history from YahooFinance\n",
    "tickers = data_read.get_sp500_ticker_names()\n",
    "symbols = yf.Tickers(\" \".join(tickers.keys()))\n",
    "market_data = symbols.history(interval = \"1d\", start=\"2006-10-20\", end=\"2013-11-20\", actions= False)\n",
    "market_data.index = pd.to_datetime(market_data.index)\n",
    "\n",
    "# Drop all days without trading, and all NaN columns from dataframe retrieved by yahoofinance.\n",
    "# Get the \n",
    "market_data = market_data.dropna(axis=0, how = \"all\")\n",
    "market_data= market_data.dropna(axis=1, how = \"all\")\n",
    "_, symbols = zip(*market_data.columns)\n",
    "traded_symbols = set(symbols)\n",
    "\n",
    "# Add daily change % for each stock (EOD price)/(Start of day price) column to the dataframe. \n",
    "for sym in traded_symbols:\n",
    "    price_ratio  = market_data[('Close',sym)] / market_data[('Open',sym)]\n",
    "    price_ratio = 100 * (price_ratio -1)\n",
    "    market_data[(\"Change\", sym)] = price_ratio \n",
    "\n",
    "# Calculate SP500 unweighted index (it is the average price of all the stocks in the S&P500) '''\n",
    "snp_index = market_data[\"Change\"].sum(axis=1) / market_data[\"Change\"].notna().sum(axis=1)\n",
    "market_data[(\"Change\",\"SNP_INDX\")] = snp_index\n",
    "\n",
    "'''\n",
    "Build labeled news dataset:\n",
    "Reterieve only the reuters news that mention companies from S&P500. For each news header keep it only\n",
    "if the price of the company it talks about changes by more then +-2%. Label the obes than rose\n",
    "more then two percent with 1 (positive), and the others with 0 (negative)\n",
    "'''\n",
    "relevant_news = data_read.get_relevant_news(traded_symbols, tickers, headers)\n",
    "for news_item in relevant_news:\n",
    "    date, symbol, _, _ = news_item\n",
    "    if pd.to_datetime(date) not in market_data.index:\n",
    "        news_item[2] = None \n",
    "        \n",
    "    else:\n",
    "        print(\"date {} sym {} \".format(date, symbol))\n",
    "        ticker_day_change = market_data.loc[pd.to_datetime(date) ,(\"Change\",symbol)]\n",
    "        sp_index_change = market_data.loc[pd.to_datetime(date) ,(\"Change\",\"SNP_INDX\")]\n",
    "        if  pd.notna(ticker_day_change) and (\n",
    "                (ticker_day_change > 2 and sp_index_change < 1.2 ) \n",
    "                or \n",
    "                (ticker_day_change < -2 and sp_index_change > -1.2) \n",
    "            ) :\n",
    "            news_item[2] = ticker_day_change\n",
    "\n",
    "reuters_labeled = pd.DataFrame(relevant_news, columns = [\"date\", \"symbol\", \"day_change\", \"header\"])\n",
    "reuters_labeled.set_index(\"date\", inplace=True)\n",
    "reuters_labeled = reuters_labeled.dropna(axis=0, how = \"any\")\n",
    "reuters_labeled = reuters_labeled[ (reuters_labeled[\"day_change\"] > 0) | (reuters_labeled[\"day_change\"] < 0)] \n",
    "reuters_labeled.loc[reuters_labeled[\"day_change\"] > 0, 'day_change'] = 1\n",
    "reuters_labeled.loc[reuters_labeled[\"day_change\"] < 0, 'day_change'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=2)\n",
    "reuters_model = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_headers = reuters_labeled['header']\n",
    "labels = reuters_labeled['day_change']\n",
    "\n",
    "ticker_re = preprocessing.get_ticker_re()\n",
    "reuters_headers = []\n",
    "for header in labeled_headers:\n",
    "        header = preprocessing.remove_tickers(ticker_re, header)\n",
    "        header = preprocessing.NER_processing(header)\n",
    "        reuters_headers.append(header)\n",
    "    \n",
    "docs_train, docs_test, y_train, y_test = train_test_split(reuters_headers, labels, test_size = 0.2, random_state = 3)\n",
    "## Fine-tune bert on Autolabeled dataset\n",
    "\n",
    "y_train = [int(y) for y in y_train]\n",
    "y_test = [int(y) for y in y_test]\n",
    "\n",
    "dataset_train = {}\n",
    "dataset_train['labels'] = y_train\n",
    "dataset_train['phrase'] = docs_train\n",
    "dataset_train = Dataset.from_dict(dataset_train)\n",
    "dataset_train = dataset_train.map(\n",
    "    lambda examples:  tokenizer(examples[\"phrase\"], padding=\"max_length\", max_length=100, truncation = True),\n",
    "    batched = True\n",
    ")\n",
    "\n",
    "print(dataset_train)\n",
    "#print(dataset_train['labels'])  #'token_type_ids', 'attention_mask'\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metrics.accuracy_score(predictions, labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=reuters_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = pipeline(\"sentiment-analysis\", model = reuters_model, \n",
    "                   tokenizer=tokenizer,max_length=100, truncation=True, padding=\"max_length\")\n",
    "results = finbert(docs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.60       531\n",
      "           1       0.54      0.53      0.54       467\n",
      "\n",
      "    accuracy                           0.57       998\n",
      "   macro avg       0.57      0.57      0.57       998\n",
      "weighted avg       0.57      0.57      0.57       998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [1 if result['label'] == 'LABEL_1' else 0 for result in results]\n",
    "print(\"classification report:\\n\", metrics.classification_report(y_test,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76555c72c80d8d4a69c4ec0bb5b1922ba2358a3f5fae3d9701a735d0328ce790"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
